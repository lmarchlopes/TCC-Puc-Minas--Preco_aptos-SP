{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. 1 - Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from folium.plugins import MarkerCluster\n",
    "from folium import plugins\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "from scipy.stats import randint\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Funções aplicadas neste notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descompacta_zip(arq_descompactar):\n",
    "    with ZipFile (arq_descompactar, 'r') as zip:\n",
    "        zip.extractall()\n",
    "        zip.printdir()\n",
    "    return arq_descompactar+' Descompactado'\n",
    "\t\n",
    "\n",
    "def extract_data_bairros(data):\n",
    "    bairros = []\n",
    "    for i in data:\n",
    "        bairros.append(i.split('\\n'))\n",
    "    for i in bairros:\n",
    "        if len(i)<4:\n",
    "            bairros.remove(i)\n",
    "    \n",
    "    Distritos=[]\n",
    "    Area=[]\n",
    "    População=[]\n",
    "    Densidade_Demográfica=[]\n",
    "    for i in bairros:\n",
    "        if i[-4].find('TOTAL')==-1:\n",
    "            Distritos.append(i[-4])\n",
    "            Area.append(i[-3])\n",
    "            População.append(i[-2])\n",
    "            Densidade_Demográfica.append(i[-1])\n",
    "            \n",
    "    len(Distritos),len(Area),len(População),len(Densidade_Demográfica)\n",
    "    if len(Distritos)==len(Area)==len(População)==len(Densidade_Demográfica):\n",
    "        n = len(Distritos)\n",
    "        print('Extração de dados está CORRETA\\nCada lista possui',n, 'elementos')\n",
    "        \n",
    "    else:\n",
    "        print('Extração de dados NÃO está correta!')\n",
    "        \n",
    "    # Criação do Dataframe\n",
    "    \n",
    "    colunas = ['Distrito', Area[0],População[0],Densidade_Demográfica[0],]\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(Distritos, Area,População,Densidade_Demográfica)), columns = colunas)\n",
    "    df.drop(index=0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "#Função converte colunas numéricas de um Dataframe para logaritno natural: Precisa informar colunas\n",
    "def simetric(df, cols):\n",
    "    cols_ln=[]\n",
    "    for col in cols:\n",
    "        sk = df[col].skew()\n",
    "        if (sk>1 or sk<-1):\n",
    "            print('Assimetria (Skewness) em %s é %.2f' %(col,sk))\n",
    "            nova_coluna = col+'_ln'\n",
    "            df[nova_coluna]=(np.log(df[col])).replace(-np.inf, 0)\n",
    "            sk_ln = df[nova_coluna].skew()\n",
    "            print('Assimetria (Skewness) em %s é %.2f' %(nova_coluna,sk_ln))\n",
    "            if sk_ln > sk:\n",
    "                df.drop(columns = nova_coluna, inplace = True)\n",
    "                print('Coluna %s Não alterada\\n' %(col))\n",
    "            else:\n",
    "                df[col]=df[nova_coluna]\n",
    "                df.drop(columns = nova_coluna, inplace = True)\n",
    "#                cols_ln.append(nova_coluna)\n",
    "                \n",
    "                print('Coluna %s ajustada \\n' %(col))\n",
    "\n",
    "                \n",
    "    return cols_ln\n",
    "\n",
    "\n",
    "# Função para calcular limites máximos e mínimos para outliers\n",
    "def outlier_iqr(df, cols):\n",
    "    for i in cols:\n",
    "        df.sort_values(by=i, ascending=True, na_position='last')\n",
    "        q1, q3 = np.nanpercentile(df[i], [25,75])\n",
    "        iqr = q3-q1\n",
    "        lower_bound = q1-(1.5*iqr)\n",
    "        upper_bound = q3+(1.5*iqr)\n",
    "        outlier_data = df[i][(df[i] < lower_bound) | (df[i] > upper_bound)] #creating a series of outlier data\n",
    "        perc = (outlier_data.count()/df[i].count())*100\n",
    "        print('Outliers em %s %.2f%% Total de registros: %.f' %(i, perc, outlier_data.count()))\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "\n",
    "\n",
    "#Cria um scatter plot com o resultado do modelo de machine learning.\n",
    "def plotar_grafico(y_t,y_pred):\n",
    "    # Visualizando as diferenças entre preços atuais e preços preditos\n",
    "    sns.set(font_scale = 1.2)\n",
    "    plt.figure(figsize = (8,5))   \n",
    "    plt.scatter(y_t, y_pred)\n",
    "    range = [y_t.min(), y_pred.max()]\n",
    "    plt.plot(range, range, 'blue')\n",
    "    plt.xlabel(\"Preços\")\n",
    "    plt.ylabel(\"Preços preditos\")\n",
    "    plt.title(\"Preços vs Preços preditos\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Checando resíduos\n",
    "    sns.set(font_scale = 1.2)\n",
    "    plt.figure(figsize = (8,5))\n",
    "    plt.scatter(y_pred,y_t-y_pred)\n",
    "    plt.plot([y_pred.min(),y_pred.max()],[0,0], 'blue')\n",
    "    plt.title(\"Predicões vs resíduos\")\n",
    "    plt.xlabel(\"Predicões\")\n",
    "    plt.ylabel(\"Resíduos\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Checando normalidade dos erros\n",
    "    sns.set(font_scale = 1.2)\n",
    "    plt.figure(figsize = (8,5))\n",
    "    sns.distplot(y_t-y_pred)\n",
    "    plt.title(\"Histograma dos Resíduos\")\n",
    "    plt.xlabel(\"Resíduos\")\n",
    "    plt.ylabel(\"Frequência\")\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coleta dos dados\n",
    "## Primeiro Dataframe - df_imoveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Chamando função para descompactar .zip\n",
    "descompacta_zip('sao-paulo-properties-april-2019.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Abertura de arquivo e visualização dos primeiros registros:\n",
    "\n",
    "csv_file = 'sao-paulo-properties-april-2019.csv'\n",
    "\n",
    "df = pd.read_csv(csv_file, sep=',')\n",
    "\n",
    "del csv_file\n",
    "display(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checando valores únicos para tipo de negociação (será extraído apenas os registros de venda)\n",
    "\n",
    "print('Tipo de negociação:', list(df['Negotiation Type'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seleção dos imóveis para venda:\n",
    "df_imoveis = df.loc[df['Negotiation Type']== 'sale']\n",
    "df_imoveis.drop(columns=['Negotiation Type'], inplace=True)\n",
    "\n",
    "# Exclusão de dados duplicados, renomeação das colunas e atualização do index\n",
    "df_imoveis = df_imoveis.drop_duplicates()\n",
    "df_imoveis.rename(columns={'Price': 'Preco', 'Size':'Area', 'Rooms': 'Quartos', 'Toilets':'Banheiros',\n",
    "                           'Parking': 'Garagem', 'Elevator': 'Elevador', 'Furnished': 'Mobiliado', 'Swimming Pool': 'Piscina',\n",
    "                           'New': 'Novo', 'District': 'Distrito', 'Property Type':'Tipo_imovel'}, inplace=True)\n",
    "\n",
    "# Como verificado, a coluna \"Distrito\" contém nome do distrito e cidade.\n",
    "# Fatiamento dessa coluna em 'Distrito' e 'Cidade': \n",
    "df_imoveis[['Distrito','Cidade']] = df_imoveis['Distrito'].str.split('/', expand = True)\n",
    "df_imoveis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Verificação se todos os registros são da cidade de São Paulo e quais os tipos de imóveis que compões o DataFrame:\n",
    "print(' Cidades no dataframe:', list(df_imoveis['Cidade'].unique()), '\\n',\n",
    "      'Tipos de imóveis: ', list(df_imoveis['Tipo_imovel'].unique()), '\\n')\n",
    "\n",
    "del df\n",
    "df_imoveis.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Como os registros são únicos, essas colunas serão excluídas\n",
    "\n",
    "df_imoveis.drop(columns=['Cidade','Tipo_imovel'], inplace=True)\n",
    "df_imoveis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Descrição sumária dos registros do Dataframe:\n",
    "display(df_imoveis.round().describe())\n",
    "# Checagem de valores Nan:\n",
    "print('\\n',\"Total de valores NaN: \", df_imoveis.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualização da distribuição dos dados do Dataframe\n",
    "\n",
    "df_hist = df_imoveis[['Preco', 'Condo', 'Area', 'Quartos', 'Banheiros', 'Suites', 'Garagem']]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(12, 8))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in df_hist.items():\n",
    "    sns.histplot(v, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0., w_pad=0.5, h_pad=5.0)\n",
    "\n",
    "del df_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segundo Dataframe - df_bairros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para buscar dados dos bairros e subprefeituras da cidade de São Paulo:\n",
    "# <'https://www.prefeitura.sp.gov.br/cidade/secretarias/subprefeituras/subprefeituras/dados_demograficos/index.php?p=12758'>\n",
    "\n",
    "# Uso de Beautiful Soup\n",
    "\n",
    "url = 'https://www.prefeitura.sp.gov.br/cidade/secretarias/subprefeituras/subprefeituras/dados_demograficos/index.php?p=12758'\n",
    "html_text = requests.get(url).text\n",
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "title = soup.find('tbody').get_text()\n",
    "data=title.strip().split('\\n\\n')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação do Dataframe df_bairros\n",
    "\n",
    "df_bairros = extract_data_bairros(data)\n",
    "\n",
    "df_bairros.to_csv(\"df_bairros.csv\")\n",
    "\n",
    "type(df_bairros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_bairros = pd.read_csv('bairros_SP.csv', index_col =0)\n",
    "df_bairros.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Criação do Dataframe df_bairros      \n",
    "display(df_bairros.head())\n",
    "display(df_bairros.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de função lambda e .replace para mudar vírgulas por pontos e pontos e vírgulas por espaço vazio\n",
    "\n",
    "df_bairros['Área (km²)'] = df_bairros['Área (km²)'].apply(lambda x: float(x.replace(\",\",\".\")))\n",
    "df_bairros['População (2010)'] = df_bairros['População (2010)'].apply(lambda x: float(x.replace(\".\",\"\")))\n",
    "df_bairros['Densidade Demográfica (Hab/km²)'] = df_bairros['Densidade Demográfica (Hab/km²)'].apply(lambda x: float(x.replace(\",\",\"\").replace(\".\",\"\")))\n",
    "\n",
    "# Visualizando as 'pontas' do Dataframe\n",
    "display(df_bairros.head())\n",
    "display(df_bairros.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##df_bairros = pd.read_csv('bairros_SP.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Descrição sumária do Dataframe:\n",
    "display(df_bairros.info())\n",
    "display(df_bairros.describe())\n",
    "print('\\n',\"Valores NaN:\",'\\n\\n', df_bairros.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualização da distribuição dos dados do Dataframe\n",
    "df= df_bairros[['Área (km²)','População (2010)', 'Densidade Demográfica (Hab/km²)']]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=1, figsize=(20, 5))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in df.items():\n",
    "    sns.histplot(v, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.2, w_pad=0.3, h_pad=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terceiro Dataframe - df_rendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Abrindo arquivo .xlsx e renomeando colunas\n",
    "\n",
    "df_rendas = pd.read_excel('Domicilios_faixa_rendimento_sal_minimos_2010.xls', skiprows=6, skipfooter=5)\n",
    "\n",
    "df_rendas.rename(columns = {'Unnamed: 0' : 'Distrito', 'Unnamed: 1': 'Total_domicilios',\n",
    "                            \"Sem rendimento (3)\": \"Sem_rendimento\"}, inplace=True)\n",
    "\n",
    "# Renomeando a última coluna:\n",
    "cols = df_rendas.columns\n",
    "for col in cols:\n",
    "    if col.startswith('Sem'):\n",
    "        df_rendas.rename(columns = {col:\"Sem_rendimento\"}, inplace=True)\n",
    "df_rendas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as 'pontas' do Dataframe\n",
    "display(df_rendas.head())\n",
    "display(df_rendas.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Descrição sumária do Dataframe:\n",
    "display(df_rendas.info())\n",
    "display(df_rendas.describe())\n",
    "df_rendas.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correção Registro Distrito 'São Miguel' e 'Moóca'\n",
    "\n",
    "df_rendas.iloc[[105],[0]]='São Miguel'\n",
    "\n",
    "df_rendas.loc[df_rendas['Distrito'] == 'Moóca', 'Distrito']='Mooca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclusão de registros manualmente.\n",
    "#Ex.: 2 Registros Butantã: um é o distrito e o outro se refere a Subprefeitura\n",
    "index_drop= [0,1,5,11,15,19,23,28,31,34,37,41,44,49,51,54,61,64,71,74,79,82,87,91,95,99,103,107,116,120,124]\n",
    "\n",
    "df_rendas.drop(index=index_drop, inplace=True)\n",
    "df_rendas.drop_duplicates(inplace=True)\n",
    "# Correção de pontos e vírgulas para números e conversão para percentagem\n",
    "cols = df_rendas.columns\n",
    "\n",
    "for col in cols:\n",
    "    if (col != 'Distrito'):\n",
    "        df_rendas[col] = df_rendas[col].apply(lambda x: float(x.replace(\".\",\"\")))\n",
    "\n",
    "print(\"Total de registros df_rendas: \",len(list(df_rendas['Distrito'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversão para percentagem\n",
    "        \n",
    "for col in cols:\n",
    "    if (col != 'Distrito') and (col != 'Total_domicilios'):\n",
    "        df_rendas[col+'_SM_%'] = round(df_rendas[col]/df_rendas['Total_domicilios']*100,2)\n",
    "        df_rendas.drop(columns=col, inplace=True)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Descrição sumária do Dataframe e checagem de valores Nan:\n",
    "display(df_rendas.info())\n",
    "display(df_rendas.describe())\n",
    "print(\"Total de valores NaN: \", df_rendas.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primeira junção de Dataframes:\n",
    "\n",
    "### df_bairros e df_rendas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge dos Dataframes df_bairros e df_rendas, inner join e contagem do número de registros\n",
    "df_distritos = pd.merge(df_bairros, df_rendas, on='Distrito', how='inner')\n",
    "print(' Total de registros em df_distritos: ', len(df_distritos), '\\n',\n",
    "      'Total de registros em df_bairros:   ',len(df_bairros),'\\n',\n",
    "      'Total de registros em df_rendas:   ',len(df_rendas))\n",
    "\n",
    "# Checar Nan's nas colunas e percentual de Nan's em cada coluna\n",
    "print(\" Total de Valores NaNs:               \", df_distritos.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df_distritos.head())\n",
    "df_distritos.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segunda junção de Dataframes:\n",
    "\n",
    "### df_imoveis e df_distritos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Merge Dataframes df_imoveis e df_distritos e posterior verificação pois a chave \"Distrito\" em df_imoveis pode conter erros\n",
    "\n",
    "df = pd.merge(df_imoveis, df_distritos, on = 'Distrito', how = 'left')\n",
    "l_distr = df['Distrito'].loc[df['População (2010)'].isna()].unique()\n",
    "print(' Total de Distritos não identificados:', len(l_distr),'\\n','Distritos não identificados: ', l_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correção gramatical dos nomes dos distritos\n",
    "\n",
    "df_distritos.loc[df_distritos['Distrito'] == 'Guaianases', 'Distrito']='Guaianazes'\n",
    "\n",
    "df_imoveis.loc[df_imoveis['Distrito'] == 'Jardim São Luis', 'Distrito']='Jardim São Luís'\n",
    "df_imoveis.loc[df_imoveis['Distrito'] == 'Medeiros', 'Distrito']='Vila Medeiros'\n",
    "\n",
    "# correção de lugares:df_imoveis com local identificado pelo nome da vila ou bairro e df_distritos pelo nome oficial\n",
    "df_imoveis.loc[df_imoveis['Distrito'] == 'Vila Madalena', 'Distrito']='Pinheiros'\n",
    "df_imoveis.loc[df_imoveis['Distrito'] == 'Brooklin', 'Distrito']='Itaim Bibi'\n",
    "df_imoveis.loc[df_imoveis['Distrito'] == 'Vila Olimpia', 'Distrito']='Itaim Bibi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Merge com adequação dos nomes dos Distritos\n",
    "\n",
    "df = pd.merge(df_imoveis, df_distritos, on = 'Distrito', how = 'left')\n",
    "\n",
    "l_distr = df['Distrito'].loc[df['População (2010)'].isna()].unique()\n",
    "print(' Total de Distritos não identificados:', len(l_distr),'\\n','Distritos não identificados: ', l_distr)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop = True,inplace=True)\n",
    "\n",
    "print( \" Total de valores nulos:\", df.isna().sum().sum(),'\\n Dimensão df:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletando demais dataframes:\n",
    "\n",
    "del df_bairros\n",
    "del df_rendas\n",
    "del df_imoveis\n",
    "#del df_distritos\n",
    "del url\n",
    "del html_text\n",
    "del soup \n",
    "del title\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_SP_28-03-22.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização da distribuição dos dados do Dataframe\n",
    "\n",
    "l_columns = list(df.columns[0:7])+ list(df.columns[14:])\n",
    "sns.set(font_scale = 1.5)\n",
    "hist = df[l_columns].hist(bins=15, figsize=(15,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuando a exploração dos dados: Relação do Preço com as variáveis: elevador, mobiliado, piscina e novo\n",
    "l_columns = df.columns[7:11]\n",
    "pos = 0\n",
    "plt.figure(figsize = (10,10))\n",
    "for i in l_columns:\n",
    "    pos +=1\n",
    "    plt.subplot(2,2,pos) \n",
    "    ax = sns.boxenplot(x = i , y = 'Preco',data = df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização Relação de Preço do imóvel em relação à: Condomínio, Área, Quartos, Banheiros, Suítes e Garagem\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "l_columns = df.columns[1:7]\n",
    "sns.set(font_scale = 1.2)\n",
    "pos = 0\n",
    "plt.figure(figsize = (15,8))\n",
    "for i in l_columns:\n",
    "    pos +=1\n",
    "    plt.subplot(2,3,pos)    \n",
    "    ax = sns.regplot(x = i, y = 'Preco', data = df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Continuando a exploração dos dados: Relação do Preço com as demais variáveis advindas dos dataset df_distritos:\n",
    "\n",
    "l_columns = df.columns[14:]\n",
    "pos = 0\n",
    "plt.figure(figsize = (14,16))\n",
    "for i in l_columns:\n",
    "    pos +=1\n",
    "    plt.subplot(4,3,pos)    \n",
    "    ax = sns.regplot(x = i, y = 'Preco', data = df, x_bins=50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis de localização: Latitude e Longitude\n",
    "# É esperado que formem um desenho\n",
    "ax = sns.pairplot(df, y_vars = 'Latitude', x_vars = ['Longitude'],\n",
    "                  height = 4, kind = 'reg')\n",
    "\n",
    "ax.fig.suptitle('Dispersão entre as Variáveis', fontsize=10, y=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df.groupby('Distrito')['Preco'].mean().reset_index()\n",
    "\n",
    "grouped.rename(columns={'Preco':'Preco_Medio'}, inplace=True)\n",
    "\n",
    "grouped = grouped.sort_values(by = 'Preco_Medio', ascending = False).head(5)\n",
    "plt.figure(figsize = (5,4))\n",
    "sns.set(font_scale = 1.40)\n",
    "ax = sns.barplot(x='Preco_Medio', y='Distrito', data = grouped,palette = 'plasma')\n",
    "print(\" Distritos com os maiores preço médio de apartamentos: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df.groupby('Distrito')['Preco'].mean().reset_index()\n",
    "\n",
    "\n",
    "grouped.rename(columns={'Preco':'Preco_Medio'}, inplace=True)\n",
    "\n",
    "grouped = grouped.sort_values(by = 'Preco_Medio', ascending = True).head(5)\n",
    "plt.figure(figsize = (5,4))\n",
    "sns.set(font_scale = 1.40)\n",
    "ax = sns.barplot(x='Preco_Medio', y='Distrito', data = grouped,palette = 'plasma')\n",
    "print(\" Distritos com os menores preço médio de apartamentos: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df.groupby('Distrito')['Preco'].mean().reset_index()\n",
    "\n",
    "grouped = grouped.loc[grouped['Preco']>1000000].sort_values(by = 'Preco',ascending = False)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.set(font_scale = 1.4)\n",
    "ax = sns.barplot(x='Preco', y='Distrito', data = grouped,palette = 'plasma')\n",
    "print(\"Total de distritos com preço médio acima de 1 milhão de reais: \", len(grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df.groupby('Distrito')['Preco'].mean().reset_index()\n",
    "\n",
    "grouped = grouped.loc[grouped['Preco']<250000].sort_values(by = 'Preco',ascending = False)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.set(font_scale = 1.4)\n",
    "ax = sns.barplot(x='Preco', y='Distrito', data = grouped,palette = 'plasma')\n",
    "print(\"Total de distritos com preço médio abaixo de 250 mil de reais: \", len(grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dicionário para gerar Nuvem de Palavras (worldcloud)\n",
    "\n",
    "#from PIL import Image\n",
    "# Import image to np.array\n",
    "#mask = np.array(Image.open('SP_mapas.png'))\n",
    "\n",
    "data_cloud = dict(df['Distrito'].value_counts())\n",
    "\n",
    "# gerar uma wordcloud\n",
    "wordcloud = WordCloud(background_color=\"white\", colormap = \"Greens_r\",\n",
    "                      width=1600, height=1000, max_words=93,\n",
    "                      max_font_size=250,\n",
    "                      min_font_size=3).generate_from_frequencies(data_cloud)\n",
    " \n",
    "# mostrar a imagem final\n",
    "fig, ax = plt.subplots(figsize=(14,14))\n",
    "ax.imshow(wordcloud, interpolation='bilinear')\n",
    "ax.set_axis_off()\n",
    " \n",
    "plt.imshow(wordcloud);\n",
    "#wordcloud.to_file(\"imóveis_summary.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df.groupby('Distrito')['População (2010)'].mean().reset_index()\n",
    "\n",
    "grouped = grouped.loc[grouped['População (2010)']>200000].sort_values(by = 'População (2010)',\n",
    "                                                                                      ascending = False)\n",
    "plt.figure(figsize = (8,12))\n",
    "sns.set(font_scale = 3.39)\n",
    "ax = sns.barplot(x='População (2010)', y='Distrito', data = grouped,palette = 'plasma')\n",
    "print(\"Total de distritos com Densidade Demográfica (Hab/km²) abaixo de 5000 hab/km²: \", len(grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df.groupby('Distrito')['Mais de 20_SM_%'].mean().reset_index()\n",
    "\n",
    "grouped = grouped.loc[grouped['Mais de 20_SM_%']>25].sort_values(by = 'Mais de 20_SM_%',\n",
    "                                                                                      ascending = False)\n",
    "plt.figure(figsize = (8,12))\n",
    "sns.set(font_scale = 3.39)\n",
    "ax = sns.barplot(x='Mais de 20_SM_%', y='Distrito', data = grouped,palette = 'plasma')\n",
    "print(\"Total de distritos com Densidade Demográfica (Hab/km²) abaixo de 5000 hab/km²: \", len(grouped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uso biblioteca folium usando colunas Latitudes e Longitudes informadas:\n",
    "\n",
    "#from folium.plugins import MarkerCluster\n",
    "\n",
    "m = folium.Map(\n",
    "    location=[-23.3, -46.6],\n",
    "    tiles='Stamen Toner',\n",
    "    zoom_start=8\n",
    ")\n",
    "mc = MarkerCluster()\n",
    "    \n",
    "for index, value in df.iterrows():\n",
    "  mc.add_child(folium.Marker([value['Latitude'], value['Longitude']], \n",
    "              popup=str(value['Distrito']),\n",
    "              tooltip=value['Distrito'],\n",
    "              icon=folium.Icon(icon='book'))).add_to(m) \n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento dados NaN\n",
    "### Tratamento Valor de condomínio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A princípio não há dados nulos\n",
    "\n",
    "print('Total de dados nulos: ', df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizando valores muito baixos, há valores 0 para condomínio.\n",
    "# E valores muito próximos de Zero\n",
    "\n",
    "cond_0 = len(df[df['Condo']==0])\n",
    "\n",
    "prox_0 = len(df[df['Condo']<=10].loc[df['Condo']>0])\n",
    "\n",
    "print('Total de registros com Condomínio igual a zero: ', cond_0)\n",
    "\n",
    "print('Total de registros com Condomínio próximo a zero: ', prox_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Substituindo valores iguais ou menores que 10 por None:\n",
    "\n",
    "df['Condo'] = np.where(df['Condo']<= 10,np.nan, df['Condo'])\n",
    "\n",
    "# substituir o missing pela mediana da coluna\n",
    "df['Condo'].fillna(round(df['Condo'].median()), inplace=True)\n",
    "\n",
    "\n",
    "df.Condo.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Condo'] = df['Condo'].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento Valores de Latitude e Longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando valores iguais a zero:\n",
    "\n",
    "lat_zero = len(df[['Distrito','Latitude', 'Longitude']].loc[df['Latitude']==0])\n",
    "long_zero = len(df[['Distrito','Latitude', 'Longitude']].loc[df['Longitude']==0])\n",
    "\n",
    "print('Total de registros de Latitude iguais a zero: ',lat_zero)\n",
    "print('Total de registros de Longitude iguais a zero: ',long_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso Biblioteca Nominatim para identificar a Latitude e Longitude Máxima e Mínima da cidade de São Paulo \n",
    "\n",
    "place = 'São Paulo, Região Imediata de São Paulo, Região Metropolitana de São Paulo,'\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geolocalização\")\n",
    "d_lat_sp={}\n",
    "d_long_sp={}\n",
    "location = geolocator.geocode( place)\n",
    "d_lat_sp['São Paulo']=[float(location.raw['boundingbox'][0]),float(location.raw['boundingbox'][1])]\n",
    "d_long_sp['São Paulo']= [float(location.raw['boundingbox'][2]), float(location.raw['boundingbox'][3])]\n",
    "\n",
    "print('Latitudes Mínima e Máxima:', d_lat_sp)\n",
    "print('Longitudes Mínima e Máxima:',d_long_sp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituindo os registros de posição geográfica incorretos por NaN(fora dos limites máximos e mínimos)\n",
    "\n",
    "df['Latitude'] = np.where(df['Latitude']>=d_lat_sp['São Paulo'][0],\n",
    "                                          np.where(df['Latitude']<=d_lat_sp['São Paulo'][1],\n",
    "                                         df['Latitude'],\n",
    "                                          np.nan),np.nan)\n",
    "\n",
    "df['Longitude'] = np.where(df['Longitude']>=d_long_sp['São Paulo'][0],\n",
    "                                          np.where(df['Longitude']<=d_long_sp['São Paulo'][1],\n",
    "                                         df['Longitude'],\n",
    "                                          np.nan),np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A partir do df, cria outros dois dataframes com as médias das latitudes e longitudes para cada distrito.\n",
    "# Com merge cria-se novo df (df_medias) com as medias e elimina-se os valor nulos \n",
    "# Assim obtêm-se a média de latitudes e longitudes em cada distrito para substituir os registros nulos:\n",
    "\n",
    "df_media_lat = df[['Distrito', 'Latitude']].groupby(['Distrito']).mean().reset_index()\n",
    "df_media_long = df[['Distrito', 'Longitude']].groupby(['Distrito']).mean().reset_index()\n",
    "\n",
    "df_medias = pd.merge(df_media_lat, df_media_long, on = 'Distrito', how = 'inner')\n",
    "df_medias.dropna(inplace=True)\n",
    "print('Dimensão do DataFrame df_medias: ', df_medias.shape)\n",
    "display(df_medias.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando quais os distritos não contêm posição geográfica subtraindo-os do df_medias no df,\n",
    "# resultando numa relação de distritos sem as médias das posições geográficas.\n",
    "\n",
    "places = list(set(df['Distrito'].unique())-set(df_medias['Distrito'].unique()))\n",
    "print( \"Relação de Distritos sem valores de Latitude e Longitude:\\n\\n\", places)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Com a biblioteca Nominatim, encontra-se Latitude e Longitude e insere no df_medias:\n",
    "\n",
    "places = list(set(df['Distrito'].unique())-set(df_medias['Distrito'].unique()))\n",
    "\n",
    "geolocator = Nominatim(user_agent=\"geolocalização\")\n",
    "for place in places:\n",
    "    location = geolocator.geocode( place+'- São Paulo - SP')\n",
    "    df_medias = df_medias.append({'Distrito' : place , 'Latitude' : location.latitude,\n",
    "                                  'Longitude' : location.longitude}, ignore_index=True)\n",
    "    \n",
    "df_medias.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# No dataframe df, substitui as latitudes e longitudes incorretas pelo nome do respectivo distrito\n",
    "df_medias.rename(columns={'Latitude':'media_Latitude', 'Longitude': 'media_Longitude'}, inplace=True)\n",
    "df = pd.merge(df, df_medias, on='Distrito', how='left')\n",
    "\n",
    "df[\"Latitude\"].fillna(df['media_Latitude'], inplace=True)\n",
    "df[\"Longitude\"].fillna(df['media_Longitude'], inplace=True)\n",
    "\n",
    "df.drop(columns = ['media_Latitude', 'media_Longitude'], inplace = True)\n",
    "print('Total registros de Latitude:', df.Latitude.count())\n",
    "print('Total registros de Longitude:', df.Longitude.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m = folium.Map(\n",
    "    location=[-23.5, -46.6],\n",
    "    tiles='Stamen Toner',\n",
    "    zoom_start=8\n",
    ")\n",
    "mc = MarkerCluster()\n",
    "    \n",
    "for index, value in df.iterrows():\n",
    "    mc.add_child(folium.Marker([value['Latitude'], value['Longitude']], \n",
    "              popup=str(value['Distrito']),\n",
    "              tooltip=value['Distrito'],\n",
    "              icon=folium.Icon(icon='book'))).add_to(m)  \n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordenadas=[]\n",
    "for lat,lng in zip(df.Latitude.values,df.Longitude.values):\n",
    "    coordenadas.append([lat,lng])\n",
    "m = folium.Map(location=[-23.55,-46.63], zoom_start=10)\n",
    "m.add_child(plugins.HeatMap(coordenadas))        \n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lat_zero = len(df[['Distrito','Latitude', 'Longitude']].loc[df['Latitude']==0])\n",
    "long_zero = len(df[['Distrito','Latitude', 'Longitude']].loc[df['Longitude']==0])\n",
    "\n",
    "print('Total de registros de Latitude iguais a zero:  ',lat_zero)\n",
    "print('Total de registros de Longitude iguais a zero: ',long_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1.5)\n",
    "ax = sns.pairplot(df, y_vars = 'Latitude', x_vars = ['Longitude'],\n",
    "                  height = 5, kind = 'reg')\n",
    "\n",
    "ax.fig.suptitle('Dispersão entre as Variáveis', fontsize=2, y=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exclusão de dados duplicados\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "display(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = df.copy()\n",
    "df_orig.to_csv(\"dataset_consolidado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.reset_index(drop = True,inplace=True)\n",
    "display(df.round().describe())\n",
    "#display(df.describe())\n",
    "print('Total de valores NaN: ', df.isna().sum().sum())\n",
    "print('Dimensão do Dataframe: ', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convertendo variáveis do tipo object para inteiro\n",
    "## chama função para relacionar as colunas numéricas e categóricas\n",
    "col = df.columns\n",
    "col_object=[]\n",
    "col_numeric=[]\n",
    "for i in col:\n",
    "    if df[i].dtypes in [np.object]:\n",
    "        col_object.append(i)\n",
    "    elif df[i].dtypes in [np.int64, np.float64]:\n",
    "        col_numeric.append(i)\n",
    "\n",
    "print('Variáveis tipo object: ', col_object)\n",
    "\n",
    "for col in col_object:\n",
    "    df[col] = pd.Categorical(df[col])\n",
    "    df[col] = df[col].cat.codes\n",
    "\n",
    "df.Distrito.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento variáveis Numéricas\n",
    "# Correlação das variáveis - Baixa correlação e Multicolinearidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baixa correlação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A variável Preco e a correlação com as demais variáveis\n",
    "\n",
    "df.corr()['Preco'].sort_values().plot(kind = 'bar',yticks = [-.9,-.1,0,.1,.9], mark_right=False, figsize=(14,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecionando as variáveis com correlação absoluta maior que |0.1|\n",
    "\n",
    "s_corr = df.corr()['Preco'].abs()\n",
    "\n",
    "s_corr = s_corr.loc[s_corr>0.1]\n",
    "s_corr.sort_values(ascending=False)\n",
    "l_select_cols = list(s_corr.index)\n",
    "\n",
    "print('Variáveis consideradas para verificar multicolinearidade:', l_select_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o mapa de correlação entre as variáveis\n",
    "sns.set(font_scale = 1)\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.title(\"Matriz de Correlação\", fontsize=20)\n",
    "sns.heatmap(df[l_select_cols].corr(), cbar=True, square= True, fmt='.2f', annot=True, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicolinearidade - verificando quais as variáveis preditoras estão correlacionadas\n",
    "\n",
    "df_corr = df[l_select_cols].corr()\n",
    "df_corr = pd.DataFrame(df_corr)\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seleção das variáveis preditoras correlacionadas e criação do dataframe com a respectiva correlação com Preco\n",
    "df_corr['indice'] = df_corr.index\n",
    "df_corr = df_corr.melt(id_vars='indice')\n",
    "df_corr = df_corr.sort_values(by = 'indice',\n",
    "                                  ascending=False)[(abs(round(df_corr['value'],\n",
    "                                                              2))>=.90)].loc[df_corr['indice']!= df_corr['variable']]\n",
    "\n",
    "columns = list(df_corr['indice'].unique())\n",
    "columns.append('Preco')\n",
    "df_corr_preco = pd.DataFrame(df[columns].corr()['Preco'].abs()).reset_index().sort_values(by='Preco',ascending=False)\n",
    "df_corr_preco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dentre essas variáveis, serão excluídas aquelas de menor grau de correlação em relação a variável Preco.\n",
    "# Com a função .merge() serão acrescentadas colunas referentes a variável e sua correlação com a preco\n",
    "df_corr = pd.merge(df_corr, df_corr_preco, how = 'left', left_on='indice', right_on = 'index')\n",
    "df_corr = pd.merge(df_corr, df_corr_preco, how = 'left', left_on='variable', right_on = 'index')\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Agora são criadas colunas que irão selecionar quais as que tem maior correlação e as de menor correlação\n",
    "df_corr[\"manter\"] = np.where(df_corr['Preco_x']>df_corr['Preco_y'],df_corr['index_x'], df_corr['index_y'])\n",
    "df_corr[\"excluir\"] = np.where(df_corr['Preco_x']<df_corr['Preco_y'],df_corr['index_x'], df_corr['index_y'])\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A seguir é aplicado método para listar quais as variáveis que devem ser mantidas e quais devem ser excluídas\n",
    "# as variáveis a serem excluidas serão subtraídas da lista de variáveis que tinham correlação maior que 0.1\n",
    "\n",
    "l_manter = list(set(df_corr[\"manter\"].unique())-set(df_corr[\"excluir\"].unique()))\n",
    "l_excluir = list(set(df_corr[\"excluir\"].unique())-set(l_manter))\n",
    "l_select_cols = list(set(l_select_cols)-set(l_excluir))\n",
    "print(l_manter)\n",
    "print('Colunas mantidas:', l_select_cols,'\\n\\nColunas a serem excluídas por Multicolinearidade:',l_excluir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotando o mapa de correlação entre as variáveis\n",
    "l_select_cols = list(df[l_select_cols].corr()['Preco'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "sns.set(font_scale = 1.5)\n",
    "plt.figure(figsize=(14,11))\n",
    "plt.title(\"Matriz de Correlação\", fontsize=20)\n",
    "sns.heatmap(df[l_select_cols].corr(), cbar=True, square= True, fmt='.2f', annot=True,\n",
    "            annot_kws={'size':15}, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando variáveis da lista l_select_cols:\n",
    "\n",
    "df=df[l_select_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_select_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df=df_orig[l_select_cols].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise variável alvo: Preço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(12,4))\n",
    "print(sns.boxplot(data=df, x='Preco'))\n",
    "\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.displot(data=df, x='Preco', kde=True,height=8)\n",
    "\n",
    "df['Preco'].describe().round(2).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criando novo dataframe com base logatmica para variáveis com distribuição assimétrica\n",
    "\n",
    "df_ln = df.copy()\n",
    "cols_ln = simetric(df_ln, l_select_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ln.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(12,4))\n",
    "print(sns.boxplot(data=df_ln, x='Preco'))\n",
    "\n",
    "sns.set(font_scale = 2)\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.displot(data=df_ln, x='Preco', kde=True,height=8)\n",
    "\n",
    "df_ln['Preco'].describe().round(2).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando o mapa de correlação entre as variáveis\n",
    "df_ln=df_ln[['Preco', 'Area', 'Garagem', 'Condo', 'Banheiros',\n",
    "        'Mais de 20_SM_%','Quartos', 'Longitude','População (2010)', 'Piscina','Densidade Demográfica (Hab/km²)']]\n",
    "\n",
    "sns.set(font_scale = 1.5)\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.title(\"Matriz de Correlação\", fontsize=20)\n",
    "sns.heatmap(df_ln.corr(), cbar=True, square= True, fmt='.2f', annot=True, annot_kws={'size':15}, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A variável Area é a que mais influencia no Preco.\n",
    "df_out = df.copy()\n",
    "df_ln_out = df_ln.copy()\n",
    "\n",
    "df['Preco_Area'] = round(df['Preco'] / df['Area'],2)\n",
    "df_ln['Preco_Area'] = round(df_ln['Preco'] / df_ln['Area'],2)\n",
    " \n",
    "sns.displot(data=df, x='Preco_Area', kde=True,height=4),\n",
    "sns.displot(data=df_ln, x='Preco_Area', kde=True,height=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "print(sns.boxplot(data=df, x='Preco_Area'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "print(sns.boxplot(data=df_ln, x='Preco_Area'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aplicação da função outlier_iqr() \n",
    "cols = ['Preco_Area']\n",
    "low_out,upp_out = outlier_iqr(df, cols)\n",
    "\n",
    "cols = ['Preco_Area']\n",
    "low_out_ln,upp_out_ln = outlier_iqr(df_ln, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cálculo do Intervalo Interquatílico (IQR) na variável Preco_Area nos datasets df e df_ln:\n",
    "\n",
    "df = df[df['Preco_Area'] > low_out]\n",
    "df = df[df['Preco_Area'] < upp_out]\n",
    "\n",
    "df_ln = df_ln[df_ln['Preco_Area'] > low_out_ln]\n",
    "df_ln = df_ln[df_ln['Preco_Area'] < upp_out_ln]\n",
    "\n",
    "\n",
    "\n",
    "df.drop(columns = ['Preco_Area'], inplace = True)\n",
    "df_ln.drop(columns = ['Preco_Area'], inplace = True)\n",
    "\n",
    "print('Shape do df:', df.shape)\n",
    "print('Shape do df_ln:', df_ln.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Aplicando RobustScaler em df e df_out\n",
    "cols = list(df.columns)\n",
    "df_r = RobustScaler().fit_transform(df)\n",
    "df_r = pd.DataFrame(df_r, columns=cols)\n",
    "df= df_r.copy()\n",
    "\n",
    "cols = list(df_out.columns)\n",
    "df_r = RobustScaler().fit_transform(df_out)\n",
    "df_r = pd.DataFrame(df_r, columns=cols)\n",
    "df_out = df_r.copy()\n",
    "\n",
    "   \n",
    "df_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Excluindo dados duplicados\n",
    "\n",
    "df_ = df.drop_duplicates()\n",
    "df_ln = df_ln.drop_duplicates()\n",
    "df_out = df_out.drop_duplicates()\n",
    "df_ln_out = df_ln_out.drop_duplicates()\n",
    "\n",
    "#Verificando se há dados nulos\n",
    "print('Totais de dados nulos\\ndf: %s \\ndf_ln: %s \\ndf_out: %s  \\ndf_ln_out: %s'\n",
    "      '\\n\\nDimensão dos DataFrames\\ndf: %s \\ndf_ln: %s \\ndf_out: %s \\ndl_ln_out:%s'\n",
    "      %(df_.isna().sum().sum(), df_ln.isna().sum().sum(), df_out.isna().sum().sum(), df_ln_out.isna().sum().sum(),\n",
    "        df.shape,df_ln.shape, df_out.shape, df_ln_out.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.to_csv('dataset_processado_SP.csv')\n",
    "df_ln.to_csv('dataset_processado_SP_ln.csv')\n",
    "df_out.to_csv('dataset_processado_SP_out.csv')\n",
    "df_ln_out.to_csv('dataset_processado_SP_ln_out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRIAÇÃO E AVALIAÇÃO DE MODELOS DE MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ = pd.read_csv('dataset_processado_SP.csv', index_col=0)\n",
    "#df_ln = pd.read_csv('dataset_processado_SP_ln.csv', index_col=0)\n",
    "#df_out = pd.read_csv('dataset_processado_SP_out.csv', index_col=0)\n",
    "#df_ln_out = pd.read_csv('dataset_processado_SP_ln_out.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reg_Linear(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']   \n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    parameters = {'n_jobs':[-1,1,2,3,5,10,25], \"fit_intercept\": [True, False],'normalize':[False, True]}\n",
    "    # define modelo/ estimador\n",
    "    model = LinearRegression()    \n",
    "    # definindo Rand search\n",
    "    rand_cv= RandomizedSearchCV(model, param_distributions=parameters, scoring='neg_mean_squared_error',\n",
    "                                cv=kf, random_state=rs)    \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x,train_y)   \n",
    "    scores = cross_val_score(model, x, y, cv=kf)   \n",
    "    d_scores={}\n",
    "    d_scores['Regressão_Linear'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "   \n",
    "    # melhor estimador\n",
    "    print(\"Melhor classificação :\", rand_cv.best_estimator_)    \n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100   \n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]} \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função Ridge\n",
    "def Ridge_(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']   \n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    parameters = {'alpha':[0.0001,0.001,0.01,0.05,0.1,1, 10,100],'max_iter':[10,20,50,100,200,300,500,700,1000,2000,5000,10000],\n",
    "                  'normalize':[False, True], 'fit_intercept':[False, True]}\n",
    "    # define modelo/ estimador\n",
    "    model = Ridge()\n",
    "    \n",
    "    # definindo Rand search\n",
    "    rand_cv= RandomizedSearchCV(model, param_distributions=parameters,scoring='neg_mean_squared_error',\n",
    "                                cv=kf, random_state=rs)\n",
    "    \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x,train_y)    \n",
    "    scores = cross_val_score(model, x, y, cv=kf)\n",
    "    d_scores={}\n",
    "    d_scores['Ridge'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "    # melhor estimador\n",
    "    print(\"Melhor classificação :\", rand_cv.best_estimator_)   \n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100   \n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]} \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lasso_(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']       \n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    parameters = {'alpha':[0.0001,0.001,0.01,0.05,0.1,1, 10,100],'max_iter':[10,20,50,100,200,300,500,700,1000,2000,5000,10000],\n",
    "                  'normalize':[False, True], 'fit_intercept':[False, True]}\n",
    "    \n",
    "    # define modelo/ estimador\n",
    "    model = Lasso()    \n",
    "    # definindo Rand search\n",
    "    rand_cv= RandomizedSearchCV(model, param_distributions=parameters,scoring='neg_mean_squared_error',\n",
    "                                cv=kf, random_state=rs)  \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x,train_y) \n",
    "    scores = cross_val_score(model, x, y, cv=kf)\n",
    "    d_scores = {}\n",
    "    scores_map['Lasso'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "    print(scores)\n",
    "    # melhor estimador\n",
    "    print(\"Melhor classificação :\", rand_cv.best_estimator_)\n",
    "    \n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100    \n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]} \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elastic_Net(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    parameters = {'alpha':[0.0001,0.001,0.01,0.05,0.1,1, 10,100],'max_iter':[10,20,50,100,200,300,500,700,1000,2000,5000,10000],\n",
    "                  'l1_ratio': np.arange(0.0, 1.0, 0.1),'fit_intercept':[False, True],'normalize':[False, True]}\n",
    "    # define modelo/ estimador\n",
    "    model = ElasticNet()\n",
    "    \n",
    "    # definindo Rand search\n",
    "    rand_cv= RandomizedSearchCV(model, param_distributions= parameters,cv=kf, random_state=rs)   \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x,train_y)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv=kf)\n",
    "    d_scores={}\n",
    "    d_scores['ElasticNet'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "    \n",
    "    # melhor estimador\n",
    "    print(\"Melhor classificação :\", rand_cv.best_estimator_)    \n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100\n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]} \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Forest_Regressor(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    \n",
    "    parameters={'n_estimators':[50,100,150,200], 'max_depth':[1,2,3,5,6,7,8,9,10,15,20,None],\n",
    "                'min_samples_leaf':randint(32,128), \"min_samples_split\":randint(32,128)}  \n",
    "    # define modelo/ estimador\n",
    "    model = RandomForestRegressor()   \n",
    "    # definindo Rand search\n",
    "    rand_cv= RandomizedSearchCV(model, param_distributions=parameters,\n",
    "                                scoring='neg_mean_squared_error',cv=kf, random_state=rs)\n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x,train_y)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv=kf)    \n",
    "    d_scores={}\n",
    "    d_scores['RandomForestReg'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "    \n",
    "    # melhor estimador\n",
    "    print(\"Melhor classificação :\", rand_cv.best_estimator_)    \n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100     \n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]}  \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decision_Tree_Regressor(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)   \n",
    "    parameters = {'max_depth':[3,5,10,15,20,30,None],'min_samples_leaf':randint(32, 128),\n",
    "                  \"min_samples_split\":randint(32,128)}\n",
    "    \n",
    "    model = DecisionTreeRegressor()\n",
    "    rand_cv = RandomizedSearchCV(model, cv=kf, param_distributions=parameters,\n",
    "                                 scoring='neg_mean_squared_error', random_state=rs)\n",
    "    \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x, train_y)\n",
    "    print(\"Melhor classificação :\", rand_cv.best_estimator_)\n",
    "    scores = cross_val_score(model,x, y, cv=kf)\n",
    "    d_scores={}\n",
    "    d_scores['DecisionTreeReg'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "    # melhor modelo\n",
    "    \n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100\n",
    "   \n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]}\n",
    "    \n",
    "    return modelos, scores_map,resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Neighbors_Regressor(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    parameters = {\"n_neighbors\" : [2,3,4,5,6,7,8,9,10,12,15],'weights':['uniform', 'distance'],\n",
    "                  'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'], 'p':[1,2]}\n",
    "    \n",
    "    model = KNeighborsRegressor()\n",
    "    rand_cv = RandomizedSearchCV(model, cv=kf, param_distributions=parameters,\n",
    "                                 scoring='neg_mean_squared_error', random_state=rs)   \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x, train_y)\n",
    "    \n",
    "    scores = cross_val_score(model, x, y, cv=kf)\n",
    "    d_scores={}\n",
    "    d_scores['KNeighborsReg'] = scores \n",
    "    scores_map[dataset].update(d_scores)\n",
    "    \n",
    "    print(\"Melhor classificação :\", rand_cv.best_estimator_)\n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100    \n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]} \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_Boosting_Regressor(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    dataframe.dropna(inplace=True)\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    \n",
    "    model = GradientBoostingRegressor()\n",
    "    parameters={'n_estimators':[50,100, 150,200,250],\n",
    "                'learning_rate':[0.001,0.01,0.05,0.1,1,5,10],\n",
    "                \"min_samples_split\" : randint(32, 128),  \"max_depth\" : [3, 5, 10, 15, 20, 30, None],\n",
    "                \"min_samples_leaf\" : randint(32, 128)}\n",
    "    \n",
    "    rand_cv = RandomizedSearchCV(model, cv=kf, param_distributions=parameters,\n",
    "                                 scoring='neg_mean_squared_error', random_state=rs)   \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x, train_y)\n",
    "    print(\"Melhor classificação:\", rand_cv.best_estimator_)\n",
    "    scores = cross_val_score(model, x, y, cv=kf)\n",
    "    d_scores={}\n",
    "    d_scores['GradientBoostingReg'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "    \n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "    \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100 \n",
    "    \n",
    "    modelos[dataset].append(modelo)\n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test],\n",
    "                  'R2 (Acurácia) - Treina':[score_train],'R2 (Acurácia) - Teste': [score_test]}  \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost_Regressor(dataset,dataframe,modelos, scores_map, kf, rs):\n",
    "    dataframe.dropna(inplace=True)\n",
    "    x = dataframe.drop(columns='Preco')\n",
    "    y = dataframe['Preco']\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2,random_state=rs)\n",
    "    \n",
    "    model = AdaBoostRegressor()\n",
    "    parameters={'n_estimators':[50,100, 150,200,250],\n",
    "                'learning_rate':[0.001,0.01,0.05,0.1,1,5,10,100],\n",
    "                'loss':['linear', 'square', 'exponential']}\n",
    "    rand_cv = RandomizedSearchCV(model, cv=kf,param_distributions= parameters,\n",
    "                                 scoring='neg_mean_squared_error', random_state=rs)   \n",
    "    #fit no Rand search\n",
    "    rand_cv.fit(train_x, train_y)\n",
    "    print(\"Melhor classificação:\", rand_cv.best_estimator_)\n",
    "    scores = cross_val_score(model, x, y, cv=kf)\n",
    "    d_scores={}    \n",
    "    d_scores['AdaBoostReg'] = scores\n",
    "    scores_map[dataset].update(d_scores)\n",
    "    \n",
    "    # melhor modelo\n",
    "    modelo = rand_cv.best_estimator_\n",
    "    modelo.fit(train_x,train_y)\n",
    "\n",
    "    y_pred = modelo.predict(train_x)\n",
    "    # Calcula a métrica Root Mean Squared Error\n",
    "    rmse_train=sqrt(mean_squared_error(train_y,y_pred))  \n",
    "    #Calcula a métrica Mean Absolute Error\n",
    "    mae_train = mean_absolute_error(train_y,y_pred)\n",
    "    score_train = modelo.score(train_x, train_y)*100    \n",
    "   \n",
    "    modelo.fit(train_x,train_y)\n",
    "    y_pred = modelo.predict(test_x)   \n",
    "    rmse_test=sqrt(mean_squared_error(test_y,y_pred))\n",
    "    mae_test = mean_absolute_error(test_y,y_pred)    \n",
    "    score_test = modelo.score(test_x, test_y)*100\n",
    "    \n",
    "    modelos[dataset].append(modelo)\n",
    "    \n",
    "    resultados = {'Dataframe': [dataset], 'Modelo': [modelo], 'MAE - Treina':[mae_train], 'MAE - Teste':[mae_test],\n",
    "                  'RMSE - Treina': [rmse_train],'RMSE - Teste': [rmse_test], 'R2 (Acurácia) - Treina':[score_train],\n",
    "                  'R2 (Acurácia) - Teste': [score_test]}  \n",
    "    \n",
    "    return modelos, scores_map,resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando as funções de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelos={'df':[],'df_ln':[],'df_out':[],'df_ln_out':[]}\n",
    "scores_map = {'df':{},'df_ln':{},'df_out':{},'df_ln_out':{}}\n",
    "kf = KFold(n_splits=5, shuffle=True)\n",
    "rs = 2000\n",
    "\n",
    "ml_functions = ['K_Neighbors_Regressor', 'Random_Forest_Regressor','Decision_Tree_Regressor', 'Reg_Linear',\n",
    "                'Ridge_', 'Lasso_','Elastic_Net', 'AdaBoost_Regressor', 'Gradient_Boosting_Regressor']\n",
    "\n",
    "datasets = {'df':df, 'df_ln':df_ln, 'df_out':df_out, 'df_ln_out':df_ln_out}\n",
    "\n",
    "\n",
    "df_compara = pd.DataFrame(columns = ['Dataframe', 'Modelo', 'MAE - Treina', 'MAE - Teste', 'RMSE - Treina',\n",
    "                                    'RMSE - Teste', 'R2 (Acurácia) - Treina', 'R2 (Acurácia) - Teste'])\n",
    "\n",
    "for function in ml_functions:    \n",
    "    for dataset,dataframe in datasets.items():\n",
    "            modelos, scores_map,resultados = globals()[function](dataset,dataframe,modelos, scores_map, kf,rs)\n",
    "            df_mod=pd.DataFrame(data = resultados)\n",
    "            df_compara=pd.concat([df_compara,df_mod])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_compara.sort_values(by=['R2 (Acurácia) - Teste'], ascending=False, inplace=True)\n",
    "d_modelo = {df_compara.iloc[0, 0]: df_compara.iloc[0, 5]}\n",
    "df_compara['Modelos'] = df_compara['Modelo'].astype(str)\n",
    "df_compara[['Dataframe', 'MAE - Treina', 'MAE - Teste', 'RMSE - Treina',\n",
    "            'RMSE - Teste', 'R2 (Acurácia) - Treina', 'R2 (Acurácia) - Teste','Modelos']].head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compara.to_csv('resultados02-4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecionando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dfs_scores={}\n",
    "d_dfs_scores['Grad_Boost_df_ln'] = scores_map['df_ln']['GradientBoostingReg']\n",
    "d_dfs_scores['GradBoost_dfln_out'] = scores_map['df_ln_out']['GradientBoostingReg']\n",
    "d_dfs_scores['DecTree_df_ln'] = scores_map['df_ln']['DecisionTreeReg']\n",
    "d_dfs_scores['Grad_Boost_df'] = scores_map['df']['GradientBoostingReg']\n",
    "\n",
    "d_dfs_scores['RandomForest_df_ln'] = scores_map['df_ln']['RandomForestReg']\n",
    "d_dfs_scores['KNN_Regr_df_ln'] = scores_map['df_ln']['KNeighborsReg']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "df_scores_map = pd.DataFrame(d_dfs_scores)\n",
    "sns.boxplot(data=df_scores_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de resíduos - Melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Relação de modelos\n",
    "modelo_sel = [df_compara.iloc[0, 1], df_compara.iloc[5, 1]]\n",
    "\n",
    "df_ln_pred = df_ln.copy()\n",
    "\n",
    "modelo=modelo_sel[0]\n",
    "\n",
    "x = df_ln.drop(columns='Preco')\n",
    "y = df_ln['Preco']\n",
    "# Treinando o modelo\n",
    "modelo.fit(x,y)\n",
    "y_pred = modelo.predict(x)\n",
    "\n",
    "df_ln_pred['preco_predito'] = y_pred\n",
    "\n",
    "# Calcula a métrica Root Mean Squared Error\n",
    "rmse=round(sqrt(mean_squared_error(y,y_pred)),4)\n",
    "#Calcula a métrica Mean Absolute Error\n",
    "mae = round(mean_absolute_error(y,y_pred),4)\n",
    "score = round(modelo.score(x, y),4)\n",
    "\n",
    "print(\"Dataframe: df_ln\\nModelo: GradientBoostingRegressor\\nMAE:%s \\nRMSE: %s\\nR²:%s\" %(mae,rmse, score))\n",
    "plotar_grafico(y,y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=modelo_sel[1]\n",
    "\n",
    "x = df_ln.drop(columns='Preco')\n",
    "y = df_ln['Preco']\n",
    "\n",
    "# Treinando o modelo\n",
    "modelo.fit(x,y)\n",
    "y_pred = modelo.predict(x)\n",
    "\n",
    "# Calcula a métrica Root Mean Squared Error\n",
    "rmse=round(sqrt(mean_squared_error(y,y_pred)),4)\n",
    "#Calcula a métrica Mean Absolute Error\n",
    "mae = round(mean_absolute_error(y,y_pred),4)\n",
    "score = round(modelo.score(x, y),4)\n",
    "\n",
    "print(\"Dataframe: df_ln\\nModelo: RandomForestRegressor\\nMAE:%s \\nRMSE: %s\\nR²:%s\" %(mae,rmse, score))\n",
    "\n",
    "plotar_grafico(y,y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apresentação de Resultado\n",
    "## Precisão por distritos e preços médios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ln_pred.rename(columns={'Preco':'Preco_ln'}, inplace=True)\n",
    "\n",
    "df_results = df_orig[df_orig.index.isin(df_ln_pred.index)]\n",
    "\n",
    "df_results = pd.concat([df_results,df_ln_pred[['Preco_ln','preco_predito']]], axis=1)\n",
    "\n",
    "df_ln_pred.rename(columns={'Preco_ln':'Preco'}, inplace=True)\n",
    "df_ln_pred.drop(columns=['preco_predito'], inplace=True)\n",
    "\n",
    "df_results['preco_predito_x'] = np.exp(df_results['preco_predito'])\n",
    "df_results['Precisao'] = np.where(df_results['preco_predito_x']/df_results['Preco']>1,\n",
    "                                 df_results['Preco']/df_results['preco_predito_x']*100,\n",
    "                                  df_results['preco_predito_x']/df_results['Preco']*100)\n",
    "\n",
    "df_results.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df_results.groupby('Distrito')['Precisao'].mean().reset_index()\n",
    "\n",
    "df_group = grouped.copy()\n",
    "\n",
    "grouped = grouped.sort_values(by = 'Precisao', ascending = True).head()\n",
    "\n",
    "plt.figure(figsize = (5,4))\n",
    "sns.set(font_scale = 1.4)\n",
    "ax = sns.histplot(x='Precisao', y='Distrito', data = grouped,bins=100)\n",
    "print(\"Precisão: Distritos com as piores previsões\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df_results.groupby('Distrito')['Precisao'].mean().reset_index()\n",
    "\n",
    "#df_group = grouped.copy()\n",
    "\n",
    "grouped = grouped.sort_values(by = 'Precisao', ascending = False).head(5)\n",
    "\n",
    "plt.figure(figsize = (5,4))\n",
    "sns.set(font_scale = 1.40)\n",
    "ax = sns.histplot(x='Precisao', y='Distrito', data = grouped, bins=100)\n",
    "print(\"Precisão: Distritos com as melhores previsões\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Distribuição dos imóveis na cidade de São Paulo\n",
    "\n",
    "grouped = df_results.groupby('Distrito')['Preco'].mean().reset_index()\n",
    "\n",
    "df_group = pd.merge(df_group, grouped, how='inner', on='Distrito')\n",
    "\n",
    "df_group.rename(columns={'Preco':'Preco_Medio'}, inplace=True)\n",
    "df_group = pd.merge(df_group, df_medias, how='inner', on='Distrito')\n",
    "\n",
    "df_group.to_csv('ap_resultado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Criando Regras de zona\n",
    "\n",
    "df_group['Zona']= np.where(df_group.media_Longitude>df_group.media_Longitude.quantile(0.70),'Centro/Leste',\n",
    "                                    np.where(df_group.media_Latitude<df_group.media_Latitude.median(),'Sul',\n",
    "                                             'Centro/Norte'))\n",
    "df_group['Zona']= np.where(df_group.media_Longitude>df_group.media_Longitude.quantile(0.80),'Leste',df_group['Zona'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_group.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapa de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Preparando Mapa de resultados\n",
    "\n",
    "sul_group = folium.FeatureGroup(name='Sul')\n",
    "leste_group = folium.FeatureGroup(name='Leste')\n",
    "norte_group = folium.FeatureGroup(name='Centro/Norte')\n",
    "centro_leste_group = folium.FeatureGroup(name='Centro/Leste')\n",
    "m = folium.Map(location=[-23.55,-46.63], zoom_start=10)\n",
    "ms = MarkerCluster()\n",
    "ml = MarkerCluster()\n",
    "mn = MarkerCluster()\n",
    "mcl = MarkerCluster()\n",
    "for dis,acur,preco,lat,long, zone in df_group.values:\n",
    "    # coordinates to locate your marker\n",
    "    COORDINATE = [lat,long]\n",
    "    if zone == \"Sul\":\n",
    "        ms.add_child(folium.Marker(location=COORDINATE,\n",
    "                      popup=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preço Médio aptos R$ '+str(round(preco,2)),\n",
    "                      tooltip=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preço Médio aptos: R$ '+str(round(preco,2)),\n",
    "                      icon=folium.Icon(icon='book'))).add_to(sul_group)\n",
    "    elif zone == \"Leste\":\n",
    "        ml.add_child(folium.Marker(location=COORDINATE,\n",
    "                      popup=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preço Médio aptos R$ '+str(round(preco,2)),\n",
    "                      tooltip=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preço Médio aptos: R$ '+str(round(preco,2)),\n",
    "                      icon=folium.Icon(icon='book'))).add_to(leste_group)\n",
    "    elif zone == \"Centro/Norte\":\n",
    "        mn.add_child(folium.Marker(location=COORDINATE,\n",
    "                      popup=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preco Médio aptos R$ '+str(round(preco,2)),\n",
    "                      tooltip=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preco Médio aptos: R$ '+str(round(preco,2)),\n",
    "                      icon=folium.Icon(icon='book'))).add_to(norte_group)\n",
    "    else:\n",
    "        mcl.add_child(folium.Marker(location=COORDINATE,\n",
    "                      popup=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preço Médio aptos R$ '+str(round(preco,2)),\n",
    "                      tooltip=str(dis)+', Precisão '+str(round(acur,2))+ '%' +'. Preço Médio aptos: R$ '+str(round(preco,2)),\n",
    "                      icon=folium.Icon(icon='book'))).add_to(centro_leste_group)\n",
    "\n",
    "m.add_child(sul_group)\n",
    "m.add_child(leste_group)\n",
    "m.add_child(norte_group)\n",
    "m.add_child(centro_leste_group)\n",
    "# turn on layer control\n",
    "m.add_child(folium.map.LayerControl(collapsed=False))\n",
    "m.save(\"mapa_resultados.html\")\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dicionário modelos. Chaves (nomes dos dataframes) e valor (modelo de ML com os respectivos melhores parâmetros calculados)\n",
    "modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
